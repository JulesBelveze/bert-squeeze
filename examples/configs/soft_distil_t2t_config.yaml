task:
  name: distil
  strategy: t2t

general:
  do_train: true
  do_eval: false
  debug: false
  labels:
    - sadness
    - joy
    - love
    - anger
    - fear
    - surprise
  output_dir: outputs
  save_steps: 1000
  validation_every_n_epoch: 1
  evaluate_during_training: true
  get_mismated: true


train:
  num_epochs: 10
  dropout: 0.2
  objective: lsl
  smoothing: 0.1
  optimizer: adamw
  weight_decay: 0.01
  discriminative_learning: false
  learning_rates: [ 2e-5 ]
  auto_lr: false
  layer_lr_decay: 0.95
  lr_scheduler: true
  adam_eps: 1e-8
  warmup_ratio: 0.06
  warmup_steps: true
  max_grad_norm: 1.0
  accumulation_steps: 1
  eval_batch_size: 2
  training_batch_size: 2
  num_labels: 6
  alpha: 0.5
  logging_steps: 100


model:
  _target_: bert-squeeze.distillation.distiller.Distiller
  teacher_config:
    _target_: bert-squeeze.models.lt_labse.LtCustomLabse
    training_config: ${train} # for the sake of compatibility but useless as model won't be finetuned
    pretrained_model: "sentence-transformers/LaBSE"
    num_labels: ${train.num_labels}
    checkpoint_path: ../checkpoints/BER-511/checkpoints/N-Step-Checkpoint_9_4500.ckpt
    name: labse
  student_config:
    _target_: bert-squeeze.models.lt_bert.LtCustomBert
    architecture: "transformer"
    training_config: ${train} # for the sake of compatibility but useless as model won't be finetuned
    pretrained_model: "bert-base-cased"
    num_labels: ${train.num_labels}
    name: bert
  training_config: ${train}


data:
  train_batch_size: ${train.training_batch_size}
  eval_batch_size: ${train.eval_batch_size}
  _target_: bert-squeeze.data.modules.distillation_module.DistillationDataModule
  teacher_module:
    _target_: bert-squeeze.data.modules.transformer_module.TransformerDataModule
    dataset_config:
      is_local: false
      path: emotion
      split:
      text_col: text
      label_col: label
      truncate_mode: head
    tokenizer_name: ${model.teacher_config.pretrained_model}
    max_length: 256
    train_batch_size: ${train.training_batch_size}
    eval_batch_size: ${train.eval_batch_size}
  student_module:
    _target_: bert-squeeze.data.modules.transformer_module.TransformerDataModule
    dataset_config:
      is_local: false
      path: emotion
      split:
      text_col: text
      label_col: label
      truncate_mode: head
    tokenizer_name: ${model.student_config.pretrained_model}
    max_length: 256
    train_batch_size: ${train.training_batch_size}
    eval_batch_size: ${train.eval_batch_size}
  soft_data_config:
    is_local: false
    name: go_emotions
    split: raw
    text_col: text
    max_samples: 10000


neptune:
  user_name: julesbelveze
  project: bert-tricks
  tags: [ ]
  logger:
    _target_: neptune.new.integrations.pytorch_lightning.NeptuneLogger
    project: ${neptune.user_name}/${neptune.project}
    name: ${task.name}


hydra:
  run:
    dir: ./outputs/${task.name}/${task.strategy}/${now:%Y-%m-%d_%H-%M-%S}