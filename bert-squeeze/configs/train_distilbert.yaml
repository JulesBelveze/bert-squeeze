task:
  name: train

general:
  do_train: true
  do_eval: false
  debug: false
  labels:
    - sadness
    - joy
    - love
    - anger
    - fear
    - surprise
  output_dir: outputs
  save_steps: 500
  validation_every_n_epoch: 1
  evaluate_during_training: true
  get_mismated: true

train:
  training_batch_size: 8
  num_epochs: 10
  dropout: 0.2
  objective: lsl
  smoothing: 0.1
  optimizer: bertadam
  weight_decay: 0.01
  discriminative_learning: true
  learning_rates: [ 2e-5 ]
  auto_lr: false
  layer_lr_decay: 0.95
  lr_scheduler: true
  adam_eps: 1e-8
  warmup_ratio: 0.06
  warmup_steps: true
  max_grad_norm: 1.0
  accumulation_steps: 1
  eval_batch_size: 16
  logging_steps: 50

model:
  _target_: bert-squeeze.models.lt_distilbert.LtCustomDistilBert
  training_config: ${train}
  pretrained_model: "distilbert-base-uncased"
  num_labels: 6

data:
  _target_: bert-squeeze.data.modules.transformer_module.TransformerDataModule
  dataset_config:
    is_local: false
    name: emotion
    split:
    text_col: text
    label_col: label
    truncate_mode: head
  tokenizer_name: ${model.pretrained_model}
  max_length: 256
  train_batch_size: ${train.training_batch_size}
  eval_batch_size: ${train.eval_batch_size}

neptune:
  user_name: julesbelveze
  project: bert-tricks
  tags: [ ]
  logger:
    _target_: neptune.new.integrations.pytorch_lightning.NeptuneLogger
    project: ${neptune.user_name}/${neptune.project}
    name: ${task.name}

