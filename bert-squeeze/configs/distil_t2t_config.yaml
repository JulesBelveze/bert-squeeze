task:
  name: distil
  strategy: t2t

dataset:
  is_local: true
  name: conference
  split: default
  text_col: title
  label_col: label
  truncate_mode: head

teacher:
  architecture: transformer
  model_type: labse
  pretrained_model: sentence-transformers/LaBSE
  pretrained_tokenizer: ${teacher.pretrained_model}
  num_labels: 5
  max_seq_length: 300
  checkpoint_path: "../outputs/BER-184"

student:
  type: transformer
  model: bert
  pretrained_model: bert-base-multilingual-cased
  pretrained_tokenizer: ${student.pretrained_model}
  num_labels: ${teacher.num_labels}
  max_seq_length: 300

train:
  batch_size: 2
  num_epochs: 10
  dropout: 0.2
  objective: lsl
  distillation_loss: kl
  smoothing: 0.1
  optimizer: adamw
  weight_decay: 0.01
  discriminative_learning: false
  learning_rates: [ 2e-5 ]
  auto_lr: false
  layer_lr_decay: 0.95
  lr_scheduler: true
  adam_eps: 1e-8
  warmup_ratio: 0.06
  warmup_steps: true
  max_grad_norm: 1.0
  accumulation_steps: 1
  eval_batch_size: 2

distil:
  alpha: 0.5

general:
  do_train: true
  do_eval: false
  debug: false
  labels: [ ISCAS, INFOCOM, WWW, SIGGRAPH, VLDB ]
  output_dir: outputs
  save_steps: 1000
  logging_steps: 100
  validation_every_n_epoch: 1
  evaluate_during_training: true
  get_mismated: true

neptune:
  user_name: julesbelveze
  project: bert-tricks
  tags: [ ]

hydra:
  run:
    dir: ./outputs/${task.name}/${task.strategy}/${now:%Y-%m-%d_%H-%M-%S}