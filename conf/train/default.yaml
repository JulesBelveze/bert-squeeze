# @package train
adam_eps: 1e-8
accumulation_steps: 1
auto_lr: false
discriminative_learning: true
dropout: 0.2
layer_lr_decay: 0.95
learning_rates: [2e-5]
logging_steps: 50
lr_scheduler: true
max_grad_norm: 1.0
num_epochs: 10
optimizer: bertadam
objective: lsl
smoothing: 0.1
warmup_ratio: 0.06
warmup_steps: true
weight_decay: 0.01

