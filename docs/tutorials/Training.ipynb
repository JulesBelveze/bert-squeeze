{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa3659b8",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f9ffc79",
      "metadata": {},
      "source": [
        "## Training a simple Transformer model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcb790d",
      "metadata": {},
      "source": [
        "We start by training a simple Transformer model on the `Setfit/emotion` dataset. We chose `microsoft/xtremedistil-l6-h256-uncased` as it is a relatively lightweight base model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3cf8d64",
      "metadata": {},
      "source": [
        "Note: in the following sections we limit the number of training steps in the `Trainer` as it is a simple demo code but you will need to increase (or unset) the `max_steps` parameter to achieve decent performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "13021276",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/julesbelveze/Desktop/bert-squeeze/.venv/bert_squeeze/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from bert_squeeze.assistants import TrainAssistant\n",
        "from lightning.pytorch import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5cf0407",
      "metadata": {},
      "outputs": [],
      "source": [
        "config_assistant = {\n",
        "    \"name\": \"bert\",\n",
        "    \"train_kwargs\": {\n",
        "        \"objective\": \"ce\"\n",
        "    },\n",
        "    \"model_kwargs\": {\n",
        "        \"pretrained_model_name_or_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
        "        \"num_labels\": 6\n",
        "    },\n",
        "    \"data_kwargs\": {\n",
        "        \"max_length\": 64,\n",
        "        \"tokenizer_name\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
        "        \"dataset_config\": {\n",
        "            \"path\": \"Setfit/emotion\"\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ce19c2bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:root:Found value for `dataset_config.path` which conflicts with parameter `dataset_path`, usingvalue from the later.\n"
          ]
        }
      ],
      "source": [
        "assistant = TrainAssistant(**config_assistant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "790a25a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration Setfit--emotion-89147fdf376d67e2\n",
            "WARNING:datasets.builder:Reusing dataset json (/Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 365.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:root:Dataset 'Setfit/emotion' successfully loaded.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-6e5f71b3bbea4a96.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-5a6336aaca39c01e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-6f41d000bddb1cc5.arrow\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 16000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = assistant.model\n",
        "\n",
        "train_dataloader = assistant.data.train_dataloader()\n",
        "test_dataloader = assistant.data.test_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a905b1b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "basic_trainer = Trainer(\n",
        "    max_steps=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "50ec4957",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name       | Type             | Params\n",
            "------------------------------------------------\n",
            "0 | objective  | CrossEntropyLoss | 0     \n",
            "1 | encoder    | CustomBertModel  | 12.8 M\n",
            "2 | classifier | Sequential       | 67.8 K\n",
            "------------------------------------------------\n",
            "12.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "12.8 M    Total params\n",
            "51.272    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0:   0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/julesbelveze/Desktop/bert-squeeze/.venv/bert_squeeze/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                                                                              "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/julesbelveze/Desktop/bert-squeeze/.venv/bert_squeeze/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|▎                                                                                                                                                                                     | 1/500 [00:01<09:27,  1.14s/it, v_num=14]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/julesbelveze/Desktop/bert-squeeze/bert_squeeze/utils/optimizers/bert_adam.py:226: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   2%|███▌                                                                                                                                                                                 | 10/500 [00:10<08:34,  1.05s/it, v_num=14]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_steps=10` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   2%|███▌                                                                                                                                                                                 | 10/500 [00:10<08:54,  1.09s/it, v_num=14]\n"
          ]
        }
      ],
      "source": [
        "basic_trainer.fit(\n",
        "    model=model, \n",
        "    train_dataloaders=train_dataloader, \n",
        "    val_dataloaders=test_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0730f674",
      "metadata": {},
      "source": [
        "## Training FastBert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f13aa3",
      "metadata": {},
      "source": [
        "Fine-tuning a `FastBert` model is as easy as fine-tuning a regular BERT. The only difference is that you need to use the `FastBertLogic` callback. The callback is in charge of freezing the model's backbone after some steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8b5a7346",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:root:Found value for `dataset_config.path` which conflicts with parameter `dataset_path`, usingvalue from the later.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration Setfit--emotion-89147fdf376d67e2\n",
            "WARNING:datasets.builder:Reusing dataset json (/Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 554.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:root:Dataset 'Setfit/emotion' successfully loaded.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-59ba575e7730665b.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-889acfd77e30ecff.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-e076430c3e6abb98.arrow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  | Name       | Type             | Params\n",
            "------------------------------------------------\n",
            "0 | objective  | CrossEntropyLoss | 0     \n",
            "1 | embeddings | BertEmbeddings   | 7.9 M \n",
            "2 | encoder    | FastBertGraph    | 6.7 M \n",
            "------------------------------------------------\n",
            "14.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "14.7 M    Total params\n",
            "58.669    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 16000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\n",
            "Epoch 0:   2%|███▌                                                                                                                                                                                 | 10/500 [00:11<09:11,  1.13s/it, v_num=15]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_steps=10` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   2%|███▌                                                                                                                                                                                 | 10/500 [00:11<09:24,  1.15s/it, v_num=15]\n"
          ]
        }
      ],
      "source": [
        "config_assistant_fastbert = {\n",
        "    \"name\": \"fastbert\",\n",
        "    \"train_kwargs\": {\n",
        "        \"objective\": \"ce\"\n",
        "    },\n",
        "    \"model_kwargs\": {\n",
        "        \"pretrained_model_name_or_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
        "        \"num_labels\": 6\n",
        "    },\n",
        "    \"data_kwargs\": {\n",
        "        \"max_length\": 64,\n",
        "        \"tokenizer_name\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
        "        \"dataset_config\": {\n",
        "            \"path\": \"Setfit/emotion\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "fastbert_assistant = TrainAssistant(**config_assistant_fastbert)\n",
        "\n",
        "basic_trainer = Trainer(\n",
        "    max_steps=10,\n",
        "    callbacks=fastbert_assistant.callbacks\n",
        ")\n",
        "\n",
        "basic_trainer.fit(\n",
        "    model=fastbert_assistant.model, \n",
        "    train_dataloaders=fastbert_assistant.data.train_dataloader(), \n",
        "    val_dataloaders=fastbert_assistant.data.test_dataloader()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d24a3a27",
      "metadata": {},
      "source": [
        "## Training TheseusBert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d42ff26",
      "metadata": {},
      "source": [
        "Similarly, fine-tuning a `TheseusBert` model is as simple as fine-tuning a regular BERT. For `TheseusBert` you do not even need to use a callback. The submodules are indeed substituted through a scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "085d3aa0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:root:Found value for `dataset_config.path` which conflicts with parameter `dataset_path`, usingvalue from the later.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Some weights of TheseusBertModel were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['encoder.successor_layers.2.attention.self.key.weight', 'encoder.successor_layers.2.attention.output.dense.weight', 'encoder.successor_layers.3.attention.output.dense.bias', 'encoder.successor_layers.2.attention.self.key.bias', 'encoder.successor_layers.5.attention.output.dense.bias', 'encoder.successor_layers.1.attention.self.query.bias', 'encoder.successor_layers.1.attention.self.value.bias', 'encoder.successor_layers.2.attention.output.LayerNorm.weight', 'encoder.successor_layers.1.output.dense.bias', 'encoder.successor_layers.2.attention.output.dense.bias', 'encoder.successor_layers.3.intermediate.dense.bias', 'encoder.successor_layers.3.attention.self.query.weight', 'encoder.successor_layers.1.attention.self.key.weight', 'encoder.successor_layers.3.attention.output.dense.weight', 'encoder.successor_layers.4.attention.self.value.bias', 'encoder.successor_layers.2.intermediate.dense.bias', 'encoder.successor_layers.1.intermediate.dense.bias', 'encoder.successor_layers.5.output.dense.bias', 'encoder.successor_layers.0.attention.self.key.bias', 'encoder.successor_layers.1.attention.self.value.weight', 'encoder.successor_layers.5.attention.self.query.weight', 'encoder.successor_layers.0.output.LayerNorm.bias', 'encoder.successor_layers.0.attention.output.dense.bias', 'encoder.successor_layers.0.attention.output.LayerNorm.bias', 'encoder.successor_layers.4.output.LayerNorm.bias', 'encoder.successor_layers.0.attention.self.value.bias', 'encoder.successor_layers.3.attention.self.key.weight', 'encoder.successor_layers.0.output.dense.bias', 'encoder.successor_layers.3.attention.self.value.weight', 'encoder.successor_layers.1.intermediate.dense.weight', 'encoder.successor_layers.3.output.dense.weight', 'encoder.successor_layers.1.attention.output.dense.weight', 'encoder.successor_layers.0.attention.self.query.bias', 'encoder.successor_layers.2.attention.self.query.bias', 'encoder.successor_layers.5.attention.output.LayerNorm.weight', 'encoder.successor_layers.1.attention.output.dense.bias', 'encoder.successor_layers.1.attention.output.LayerNorm.weight', 'encoder.successor_layers.2.intermediate.dense.weight', 'encoder.successor_layers.1.output.LayerNorm.bias', 'encoder.successor_layers.5.attention.self.key.bias', 'encoder.successor_layers.0.attention.self.key.weight', 'encoder.successor_layers.3.attention.self.query.bias', 'encoder.successor_layers.4.attention.output.dense.weight', 'encoder.successor_layers.5.attention.self.query.bias', 'encoder.successor_layers.0.attention.output.dense.weight', 'encoder.successor_layers.5.output.dense.weight', 'encoder.successor_layers.4.attention.self.query.bias', 'encoder.successor_layers.0.attention.self.query.weight', 'encoder.successor_layers.2.output.dense.bias', 'encoder.successor_layers.0.output.LayerNorm.weight', 'encoder.successor_layers.4.output.LayerNorm.weight', 'encoder.successor_layers.3.intermediate.dense.weight', 'encoder.successor_layers.1.attention.self.key.bias', 'encoder.successor_layers.2.output.LayerNorm.bias', 'encoder.successor_layers.4.attention.self.key.bias', 'encoder.successor_layers.0.intermediate.dense.weight', 'encoder.successor_layers.1.output.dense.weight', 'encoder.successor_layers.5.output.LayerNorm.bias', 'encoder.successor_layers.4.output.dense.bias', 'encoder.successor_layers.3.attention.output.LayerNorm.bias', 'encoder.successor_layers.0.attention.self.value.weight', 'encoder.successor_layers.3.attention.self.key.bias', 'encoder.successor_layers.5.attention.output.LayerNorm.bias', 'encoder.successor_layers.4.output.dense.weight', 'encoder.successor_layers.4.attention.output.LayerNorm.weight', 'encoder.successor_layers.4.attention.self.value.weight', 'encoder.successor_layers.2.attention.self.value.bias', 'encoder.successor_layers.4.attention.output.LayerNorm.bias', 'encoder.successor_layers.5.attention.self.value.bias', 'encoder.successor_layers.0.attention.output.LayerNorm.weight', 'encoder.successor_layers.3.output.dense.bias', 'encoder.successor_layers.4.attention.output.dense.bias', 'encoder.successor_layers.3.output.LayerNorm.weight', 'encoder.successor_layers.2.output.LayerNorm.weight', 'encoder.successor_layers.4.attention.self.key.weight', 'encoder.successor_layers.5.attention.output.dense.weight', 'encoder.successor_layers.0.output.dense.weight', 'encoder.successor_layers.3.output.LayerNorm.bias', 'encoder.successor_layers.4.attention.self.query.weight', 'encoder.successor_layers.2.attention.self.value.weight', 'encoder.successor_layers.0.intermediate.dense.bias', 'encoder.successor_layers.4.intermediate.dense.weight', 'encoder.successor_layers.5.intermediate.dense.bias', 'encoder.successor_layers.5.output.LayerNorm.weight', 'encoder.successor_layers.2.attention.self.query.weight', 'encoder.successor_layers.3.attention.self.value.bias', 'encoder.successor_layers.1.attention.output.LayerNorm.bias', 'encoder.successor_layers.2.output.dense.weight', 'encoder.successor_layers.5.intermediate.dense.weight', 'encoder.successor_layers.4.intermediate.dense.bias', 'encoder.successor_layers.1.attention.self.query.weight', 'encoder.successor_layers.3.attention.output.LayerNorm.weight', 'encoder.successor_layers.5.attention.self.value.weight', 'encoder.successor_layers.1.output.LayerNorm.weight', 'encoder.successor_layers.2.attention.output.LayerNorm.bias', 'encoder.successor_layers.5.attention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration Setfit--emotion-89147fdf376d67e2\n",
            "WARNING:datasets.builder:Reusing dataset json (/Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 378.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:root:Dataset 'Setfit/emotion' successfully loaded.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-59ba575e7730665b.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-889acfd77e30ecff.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/json/Setfit--emotion-89147fdf376d67e2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-e076430c3e6abb98.arrow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  | Name       | Type             | Params\n",
            "------------------------------------------------\n",
            "0 | objective  | CrossEntropyLoss | 0     \n",
            "1 | encoder    | TheseusBertModel | 17.5 M\n",
            "2 | classifier | Sequential       | 67.8 K\n",
            "------------------------------------------------\n",
            "17.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "17.6 M    Total params\n",
            "70.226    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 16000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\n",
            "Epoch 0:   2%|███▌                                                                                                                                                                                 | 10/500 [00:10<08:34,  1.05s/it, v_num=16]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_steps=10` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   2%|███▌                                                                                                                                                                                 | 10/500 [00:10<08:51,  1.08s/it, v_num=16]\n"
          ]
        }
      ],
      "source": [
        "config_assistant_fastbert = {\n",
        "    \"name\": \"theseusbert\",\n",
        "    \"train_kwargs\": {\n",
        "        \"objective\": \"ce\"\n",
        "    },\n",
        "    \"model_kwargs\": {\n",
        "        \"pretrained_model_name_or_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
        "        \"num_labels\": 6\n",
        "    },\n",
        "    \"data_kwargs\": {\n",
        "        \"max_length\": 64,\n",
        "        \"tokenizer_name\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
        "        \"dataset_config\": {\n",
        "            \"path\": \"Setfit/emotion\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "fastbert_assistant = TrainAssistant(**config_assistant_fastbert)\n",
        "\n",
        "basic_trainer = Trainer(\n",
        "    max_steps=10\n",
        ")\n",
        "\n",
        "basic_trainer.fit(\n",
        "    model=fastbert_assistant.model, \n",
        "    train_dataloaders=fastbert_assistant.data.train_dataloader(), \n",
        "    val_dataloaders=fastbert_assistant.data.test_dataloader()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6acb07be",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bert_squeeze",
      "language": "python",
      "name": "bert_squeeze"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
