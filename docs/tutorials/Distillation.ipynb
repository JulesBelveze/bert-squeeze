{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e125fa5c",
   "metadata": {},
   "source": [
    "# Distillation + Pruning + Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8b9b8",
   "metadata": {},
   "source": [
    "The following code snippets allow you to distil a `microsoft/xtremedistil-l6-h384-uncased` teacher into a `microsoft/xtremedistil-l6-h256-uncased` student.\n",
    "\n",
    "At the end of the training the student is then [dynamically quantized](https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_dynamic.html#torch.ao.quantization.quantize_dynamic) and some weights are pruned based on their magnitude.\n",
    "\n",
    "If you do not want to perform quantization and/or pruning simply remove the corresponding callback from the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "336a318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_squeeze.assistants import DistilAssistant\n",
    "from lightning.pytorch import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0edf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using xtremedistil because they are lightweight models but feel free\n",
    "# to change it to the base model you want.\n",
    "config_assistant = {\n",
    "    \"name\": \"distil\",\n",
    "    \"teacher_kwargs\": {\n",
    "        \"pretrained_model\": \"microsoft/xtremedistil-l6-h384-uncased\",\n",
    "        \"num_labels\": 2\n",
    "    },\n",
    "    \"student_kwargs\": {\n",
    "        \"pretrained_model\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
    "        \"num_labels\": 2\n",
    "    },\n",
    "    \"data_kwargs\": {\n",
    "        \"teacher_module\": {\n",
    "            \"dataset_config\": {\n",
    "                \"path\": \"linxinyuan/cola\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"callbacks\": [\n",
    "        {\n",
    "            \"_target_\": \"bert_squeeze.utils.callbacks.pruning.ThresholdBasedPruning\",\n",
    "            \"threshold\": 0.2,\n",
    "            \"start_pruning_epoch\": -1\n",
    "        },\n",
    "        {\n",
    "            \"_target_\": \"bert_squeeze.utils.callbacks.quantization.DynamicQuantization\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73654166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset mind (/Users/julesbelveze/.cache/huggingface/datasets/linxinyuan___mind/default/0.0.0/0871d55203d4de46ef1815400998ed8f219236694f0d03786bde849741f04cd4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4441372679af408ebc992f301f4598e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataset 'linxinyuan/cola' successfully loaded.\n",
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset mind (/Users/julesbelveze/.cache/huggingface/datasets/linxinyuan___mind/default/0.0.0/0871d55203d4de46ef1815400998ed8f219236694f0d03786bde849741f04cd4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a108972772465a8ca3cefdc7f85445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataset 'linxinyuan/cola' successfully loaded.\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/linxinyuan___mind/default/0.0.0/0871d55203d4de46ef1815400998ed8f219236694f0d03786bde849741f04cd4/cache-e56c581864af9d3c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/linxinyuan___mind/default/0.0.0/0871d55203d4de46ef1815400998ed8f219236694f0d03786bde849741f04cd4/cache-dc929e775e97a506.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/linxinyuan___mind/default/0.0.0/0871d55203d4de46ef1815400998ed8f219236694f0d03786bde849741f04cd4/cache-545d3be6487c373a.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/julesbelveze/.cache/huggingface/datasets/linxinyuan___mind/default/0.0.0/0871d55203d4de46ef1815400998ed8f219236694f0d03786bde849741f04cd4/cache-3037a309a170717b.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type               | Params\n",
      "----------------------------------------------------\n",
      "0 | teacher      | LtCustomBert       | 22.9 M\n",
      "1 | student      | LtCustomBert       | 12.8 M\n",
      "2 | loss_ce      | LabelSmoothingLoss | 0     \n",
      "3 | loss_distill | MSELoss            | 0     \n",
      "----------------------------------------------------\n",
      "35.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "35.7 M    Total params\n",
      "142.718   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julesbelveze/Desktop/bert-squeeze/.venv/bert_squeeze/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/julesbelveze/Desktop/bert-squeeze/.venv/bert_squeeze/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a51ebccce5846a79029d5f64536b251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julesbelveze/Desktop/bert-squeeze/bert_squeeze/utils/optimizers/bert_adam.py:226: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "`Trainer.fit` stopped: `max_steps=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning model...\n",
      "INFO:root:Model quantized and saved - size (MB): 142.818233\n"
     ]
    }
   ],
   "source": [
    "assistant = DistilAssistant(**config_assistant)\n",
    "\n",
    "model = assistant.model\n",
    "callbacks = assistant.callbacks\n",
    "train_dataloader = assistant.data.train_dataloader()\n",
    "test_dataloader = assistant.data.test_dataloader()\n",
    "\n",
    "basic_trainer = Trainer(\n",
    "    max_steps=2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "basic_trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91065c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_squeeze",
   "language": "python",
   "name": "bert_squeeze"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
