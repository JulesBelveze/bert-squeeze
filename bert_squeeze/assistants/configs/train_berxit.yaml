general:
  debug: false
  do_train: true
  do_eval: false
  get_mismatched: true
  evaluate_during_training: true
  labels: [ 0,1 ]
  num_labels: 2
  output_dir: outputs
  save_steps: 500
  validation_every_n_epoch: 1

train:
  adam_eps: 1e-8
  accumulation_steps: 1
  auto_lr: false
  discriminative_learning: true
  # Training stage for BERxiT:
  # - "backbone": train encoder + ramps + final classifier (no gate loss unless train_gates=true)
  # - "gates": freeze backbone/ramps/classifier and train only gates
  # You can also keep "backbone" here and use `switch_step` to switch to gate training mid-run.
  train_stage: "backbone"
  # Optional global-step at which to switch from backbone to gate training within a single run.
  # If null, no automatic switch is performed.
  switch_step:
  dropout: 0.2
  layer_lr_decay: 0.95
  learning_rates: [ 2e-5 ]
  logging_steps: 100
  lr_scheduler: true
  max_grad_norm: 1.0
  num_epochs: 10
  optimizer: bertadam
  objective: lsl
  smoothing: 0.1
  warmup_ratio: 0.06
  warmup_steps: true
  weight_decay: 0.01

  train_highway: true
  early_exit_entropy: -1
  # BERxiT-specific options
  train_gates: true
  gate_hidden_dim: 32
  # Either a single float applied to all layers or a list of floats per layer
  gate_thresholds: 0.5

model:
  _target_: bert_squeeze.models.lt_berxit.LtBerxit
  training_config: ${train}
  pretrained_model: "bert-base-cased"
  num_labels: ${general.num_labels}
  scorer:
    _target_: bert_squeeze.utils.scorers.sequence_classification_scorer.BaseSequenceClassificationScorer
    labels: ${general.labels}

data:
  _target_: bert_squeeze.data.modules.transformer_module.TransformerDataModule
  dataset_config:
    is_local: false
    path:
    split:
    text_col: text
    label_col: label
    truncate_mode: head
  tokenizer_name: ${model.pretrained_model}
  max_length: 256
